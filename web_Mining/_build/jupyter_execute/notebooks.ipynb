{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "We3kl8nTuMUF"
   },
   "source": [
    "# **Data crawling**\n",
    "Data crawling adalah proses pengambilan data yang tersedia secara online untuk umum. Proses ini kemudian mengimpor informasi atau data yang telah ditemukan ke dalam file lokal di komputer Anda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAb2TIcEu-81"
   },
   "source": [
    "## Twint\n",
    "\n",
    "Twint adalah sebuah tools yang digunakan untuk melakukan scrapping dari aplikasi twitter yang disetting secara khusus menggunakan bahasa pemrograman Python. Twint dapat kita gunakan dan jalankan tanpa harus menggunakan API dari Twitter itu sendiri, dengan kapasitas scrapping data maksimalnya adalah 3200 tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3253,
     "status": "ok",
     "timestamp": 1669758849737,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "9dlXc60wYGMp",
    "outputId": "4b04b8c1-6189-4a6e-84f4-2d99c67d6e5c"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mephemeral\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m       readonly=readonly)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 125\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1669758849738,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "IvuL6s3JYM-c",
    "outputId": "69688177-81cb-442b-e0ff-e9ab278e56a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/web_mining/web_Mining\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/web_mining/web_Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23407,
     "status": "ok",
     "timestamp": 1669758873139,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "xFH7xVwxUhEC",
    "outputId": "c32e9c18-9243-495c-95b2-7497fe44670e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'twint'...\n",
      "remote: Enumerating objects: 47, done.\u001b[K\n",
      "remote: Counting objects: 100% (47/47), done.\u001b[K\n",
      "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
      "remote: Total 47 (delta 3), reused 14 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (47/47), done.\n",
      "/content/drive/MyDrive/web_mining/web_Mining/twint\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Processing /content/drive/MyDrive/web_mining/web_Mining/twint\n",
      "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
      "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (3.8.3)\n",
      "Collecting aiodns\n",
      "  Downloading aiodns-3.0.0-py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (4.6.3)\n",
      "Collecting cchardet\n",
      "  Downloading cchardet-2.1.7-cp37-cp37m-manylinux2010_x86_64.whl (263 kB)\n",
      "\u001b[K     |████████████████████████████████| 263 kB 11.2 MB/s \n",
      "\u001b[?25hCollecting dataclasses\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Collecting elasticsearch\n",
      "  Downloading elasticsearch-8.5.2-py3-none-any.whl (385 kB)\n",
      "\u001b[K     |████████████████████████████████| 385 kB 21.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pysocks in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.7.1)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (1.3.5)\n",
      "Collecting aiohttp_socks<=0.4.1\n",
      "  Downloading aiohttp_socks-0.4.1-py3-none-any.whl (17 kB)\n",
      "Collecting schedule\n",
      "  Downloading schedule-1.1.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: geopy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.17.0)\n",
      "Collecting fake-useragent\n",
      "  Downloading fake_useragent-1.1.0-py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 6.2 MB/s \n",
      "\u001b[?25hCollecting googletransx\n",
      "  Downloading googletransx-2.4.2.tar.gz (13 kB)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->-r requirements.txt (line 8)) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->-r requirements.txt (line 8)) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23.0->-r requirements.txt (line 8)) (1.21.6)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp_socks<=0.4.1->-r requirements.txt (line 9)) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->-r requirements.txt (line 1)) (2.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->-r requirements.txt (line 1)) (1.3.3)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->-r requirements.txt (line 1)) (0.13.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->-r requirements.txt (line 1)) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->-r requirements.txt (line 1)) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->-r requirements.txt (line 1)) (4.1.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.0->-r requirements.txt (line 8)) (1.15.0)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->-r requirements.txt (line 1)) (2.10)\n",
      "Collecting pycares>=4.0.0\n",
      "  Downloading pycares-4.2.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
      "\u001b[K     |████████████████████████████████| 288 kB 37.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pycares>=4.0.0->aiodns->-r requirements.txt (line 2)) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.5.0->pycares>=4.0.0->aiodns->-r requirements.txt (line 2)) (2.21)\n",
      "Collecting elastic-transport<9,>=8\n",
      "  Downloading elastic_transport-8.4.0-py3-none-any.whl (59 kB)\n",
      "\u001b[K     |████████████████████████████████| 59 kB 6.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from elastic-transport<9,>=8->elasticsearch->-r requirements.txt (line 6)) (2022.9.24)\n",
      "Collecting urllib3<2,>=1.26.2\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 31.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.7/dist-packages (from geopy->-r requirements.txt (line 11)) (1.52)\n",
      "Requirement already satisfied: importlib-metadata~=4.0 in /usr/local/lib/python3.7/dist-packages (from fake-useragent->-r requirements.txt (line 12)) (4.13.0)\n",
      "Requirement already satisfied: importlib-resources>=5.0 in /usr/local/lib/python3.7/dist-packages (from fake-useragent->-r requirements.txt (line 12)) (5.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata~=4.0->fake-useragent->-r requirements.txt (line 12)) (3.10.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from googletransx->-r requirements.txt (line 13)) (2.23.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->googletransx->-r requirements.txt (line 13)) (3.0.4)\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 764 kB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: twint, googletransx\n",
      "  Building wheel for twint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for twint: filename=twint-2.1.21-py3-none-any.whl size=38870 sha256=2e1c70ff8d5bcc99797c67c692a33f090cf16e9852427f2d2c3510f161737c12\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-63grgmvb/wheels/32/e7/72/f637d8e3152680eb550bafea6ea8ade51831260bbc1ba9708d\n",
      "  Building wheel for googletransx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for googletransx: filename=googletransx-2.4.2-py3-none-any.whl size=15968 sha256=082984e3bbee8d9fdfb1043f61bae163c602ab39204054be39ace1177a2d3add\n",
      "  Stored in directory: /root/.cache/pip/wheels/66/d5/b1/31104b338f7fd45aa8f7d22587765db06773b13df48a89735f\n",
      "Successfully built twint googletransx\n",
      "Installing collected packages: urllib3, requests, pycares, elastic-transport, schedule, googletransx, fake-useragent, elasticsearch, dataclasses, cchardet, aiohttp-socks, aiodns, twint\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.24.3\n",
      "    Uninstalling urllib3-1.24.3:\n",
      "      Successfully uninstalled urllib3-1.24.3\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.23.0\n",
      "    Uninstalling requests-2.23.0:\n",
      "      Successfully uninstalled requests-2.23.0\n",
      "Successfully installed aiodns-3.0.0 aiohttp-socks-0.4.1 cchardet-2.1.7 dataclasses-0.6 elastic-transport-8.4.0 elasticsearch-8.5.2 fake-useragent-1.1.0 googletransx-2.4.2 pycares-4.2.2 requests-2.28.1 schedule-1.1.0 twint-2.1.21 urllib3-1.26.13\n"
     ]
    }
   ],
   "source": [
    "!git clone --depth=1 https://github.com/twintproject/twint.git\n",
    "%cd twint\n",
    "!pip3 install . -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3856,
     "status": "ok",
     "timestamp": 1669758876985,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "m2K-DUnBDdU_",
    "outputId": "b2ba1680-28b2-441b-99fb-55adafff0d80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting nest-asyncio\n",
      "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
      "Installing collected packages: nest-asyncio\n",
      "Successfully installed nest-asyncio-1.5.6\n"
     ]
    }
   ],
   "source": [
    "!pip install nest-asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5704,
     "status": "ok",
     "timestamp": 1669758882680,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "MkvP5Mt3DeJ1",
    "outputId": "1b4b1ce4-11b3-42b3-9919-8e75ab531d03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting aiohttp==3.7.0\n",
      "  Downloading aiohttp-3.7.0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 13.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.7.0) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.7.0) (22.1.0)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.7.0) (3.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp==3.7.0) (1.8.1)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp==3.7.0) (4.1.1)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp==3.7.0) (2.10)\n",
      "Installing collected packages: async-timeout, aiohttp\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 4.0.2\n",
      "    Uninstalling async-timeout-4.0.2:\n",
      "      Successfully uninstalled async-timeout-4.0.2\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.8.3\n",
      "    Uninstalling aiohttp-3.8.3:\n",
      "      Successfully uninstalled aiohttp-3.8.3\n",
      "Successfully installed aiohttp-3.7.0 async-timeout-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install aiohttp==3.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3640,
     "status": "ok",
     "timestamp": 1669758886312,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "D-A1_9INDgUQ",
    "outputId": "29372fa0-377e-44bc-da99-741de0935277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1597710346269384704 2022-11-29 21:53:25 +0000 <tkgit1> @WagimanDeep212_ ya udah @prabowo + @ganjarpranowo\n",
      "1597709912494469120 2022-11-29 21:51:41 +0000 <SutrisN16480678> @prabowo Salam sejahtera BPK\n",
      "1597709801215361026 2022-11-29 21:51:15 +0000 <tjandra_alwi> Anda coba telusuri deh.janji anies ke prabowo, janji anies Rumah DP 0 persen dan Janji buat bahagia warga jakarta !! BOHOOOOONG KHAN??\n",
      "1597709747431837696 2022-11-29 21:51:02 +0000 <sufriyadi83> Alun-alun Nganjuk  #NganjukKotaAngin  TPPJatim #TPPSumenep #TPPSaronggi  @jokowi  @halimiskandarnu @yennywahid @prabowo @bclsinclair @NajwaShihab @Ansor_Satu @Lesti_Jenong_DA @mohmahfudmd @gusmusgusmu @ganjarpranowo @agnezmo @AgusYudhoyono @MuhaiminIskand  https://t.co/GZLqYynTK6\n",
      "1597708818720256000 2022-11-29 21:47:21 +0000 <lienadap> Alun-alun Nganjuk  #NganjukKotaAngin  TPPJatim #TPPSumenep #TPPSaronggi  @jokowi  @halimiskandarnu @yennywahid @prabowo @bclsinclair @NajwaShihab @Ansor_Satu @Lesti_Jenong_DA @mohmahfudmd @gusmusgusmu @ganjarpranowo @agnezmo @AgusYudhoyono @MuhaiminIskand  https://t.co/J0U9nhs5MX\n",
      "1597708442096963584 2022-11-29 21:45:51 +0000 <Jokorephidu> Pak prabowo rambut kpl nya item... Atau mgkn rambut lain yg putih....\n",
      "1597708339000901634 2022-11-29 21:45:26 +0000 <muhammadsyakur2> @VIVAcoid Nah loh, Prabowo ga bisa ama cak Imin, mau ga mau Prabowo-Puan kl mau lanjut, kalo ga ya ga bisa nyalon, bakalan duel Anies vs Ganjar\n",
      "1597708256675508225 2022-11-29 21:45:07 +0000 <DiptraD> @kadrunmampos Tahun 2014 dan 2019 semua HARAM mengenakan simbol kafir dan masuk ke Gereja KECUALI Prabowo  Tahun 2021 - 2024 semua HARAM mengenakan simbol kafir dan masuk ke Gereja KECUALI anoes  Tahun \"XXXX\" semua HARAM mengenakan simbol kafir dan masuk ke Gereja KECUALI calon presiden kadrun\n",
      "1597707888184565761 2022-11-29 21:43:39 +0000 <gelmbel9018> @hardye76 @irjenmahasiwa Hih pks takut ama pengaruh jokowi\n",
      "1597707789899472896 2022-11-29 21:43:15 +0000 <Muttaqi20081332> @tvOneNews Hahaha bisanya hanya ngeles dan kasih PHP ke Pak Prabowo. Lantas bijimana dgn nasib Puan Maharani dan Airlangga Hartarto. Negarawan koq Clometan. Hari ini kasih Pete besok kasih Jengkol.🤣🤣🤣\n",
      "1597707744491950080 2022-11-29 21:43:04 +0000 <WulandaPrabowo> Asli ne hotel seremmm keleussss ,, ohmygod ,, (˘_˘\")\"\n",
      "1597707686329864192 2022-11-29 21:42:51 +0000 <Artikha> Ya pastilah Anies melejit yg survey Rico Marbun orang PKS, dulu Prabowo juga menang disurvey orang ini, tapi Jokowi yg jadi Presiden\n",
      "1597707665014083584 2022-11-29 21:42:45 +0000 <fideldapati> Jokowi: Pak Prabowo Punya Kerutan di Wajah dan Rambut Putih Klik untuk baca:  https://t.co/ShM4S1066H  https://t.co/aIyLdqv6jd\n",
      "1597707382536433665 2022-11-29 21:41:38 +0000 <rasyidtonitan> Skrg seolah olah bahwa gub terbodoh di fitnah ,,hmmmm loloh 2,1 % kog ngeyel ,,bagi saya selama pak prabowo dan bp ganjar saya pilih asli pribumi ngapain inport perusak\n",
      "1597706571739299841 2022-11-29 21:38:25 +0000 <CNNIndonesia> Survei Capres Median: Prabowo, Anies, Ganjar Bersaing Ketat  https://t.co/Z2GiTehpdB\n",
      "1597704624517316608 2022-11-29 21:30:41 +0000 <AnakSDManja> 2Tanpa rasa takut menghalu rintangan HP_Don_ Hadi Prabowo #HPDonMenang #PKS3Besar\n",
      "1597704341129154560 2022-11-29 21:29:33 +0000 <ekojhones7> Dah tua ya berkerut , kecuali oprasi plastik seumur prabowo gak berkerut\n",
      "1597704324276449280 2022-11-29 21:29:29 +0000 <gelmbel9018> @KakaKeyo Aws air laut pasang\n",
      "1597704234602233859 2022-11-29 21:29:08 +0000 <Otakuntilanak48> Gokigen Naname na Mermaid\n",
      "1597704076200140803 2022-11-29 21:28:30 +0000 <gelmbel9018> @opposite6892 Cinta buta tu nama nya .!!org tua kita dl yg seharus nya di dulu kan\n",
      "1597703813028540416 2022-11-29 21:27:27 +0000 <YPardjono> Ada orang ini Elektabilitas Pak Prabowo Nggemboooissss\n",
      "1597703495318405120 2022-11-29 21:26:11 +0000 <phattyupu> @GerakAnies Insya Allah Prabowo\n",
      "1597703304909180929 2022-11-29 21:25:26 +0000 <phattyupu> @GerakAnies prabowo dong\n",
      "1597703250735943681 2022-11-29 21:25:13 +0000 <sayids> @z4r4n @benny_rhamdani_ Oh..ternyata juga melaporkan @prabowo dan @fadlizon yang katanya gajinsetahun 25 Milyar tapi sekarang mereka 1 gerbong, jadi aman ga ada  tuntutan balik\n",
      "1597702397626753024 2022-11-29 21:21:50 +0000 <gelmbel9018> @Simbok_Dharmi Lolo isuk” ws uplod sarapan marah i luwe yuk🤦\n",
      "1597701727582507008 2022-11-29 21:19:10 +0000 <gelmbel9018> @DaengWahidin2 Kok goblok mereka punya sekil dan sekil kt hanya demo melulu 🤣🤣🤣\n",
      "1597700782077661186 2022-11-29 21:15:24 +0000 <BattoAjax> Wah pak @prabowo  MARAH diginiin!!! Siapa itu ANIES???  Ayoo GERINDRA KOALISI dengan PKS, maka ANIES SELESAI.  Anies Penghianat @Gerindra\n",
      "1597700179368763392 2022-11-29 21:13:01 +0000 <WulandaPrabowo> Pagi2 itu nton garfield oweee ihiyyy ~(‾▿‾~)(~‾▿‾)~\n",
      "1597698938479710208 2022-11-29 21:08:05 +0000 <yudhady28> Mantap!  Nasional Tempo: Kepala BIN Budi Gunawan: Kerutan Pemimpin Versi Jokowi 100 Persen Identik dengan Prabowo.  https://t.co/IwixkGpa6H  via @GoogleNews\n",
      "1597698243827470336 2022-11-29 21:05:19 +0000 <asdfkg_unknown> pak @prabowo saya boleh join jadi admin @Gerindra juga gk? 😭🙏🏻\n",
      "1597698096817123328 2022-11-29 21:04:44 +0000 <murphy666_> pak prabowo capres tersadboy 2024.\n",
      "1597697218374340610 2022-11-29 21:01:15 +0000 <123gunawanqq> @lampungpost_ Kenapa bos ponzi wahyu kenzo Atg 0.5 auto trade gold gak ditangkap ? Yang sudah merugikan orang banyak . Siapa yang dibelakang wahyu kenzo sebagai beking nya ? @jokowi @prabowo  @Dennysiregar7 @CCICPolri\n",
      "1597697150699261952 2022-11-29 21:00:59 +0000 <AnakSDManja> 1Barisan Mujahid melangkah ke depan HP_Don_ Hadi Prabowo #HPDonMenang #PKS3Besar\n",
      "1597695566620004352 2022-11-29 20:54:41 +0000 <DidyCare> ՇիՅ շիՅ շիՅ....keterangan tertulis @pangisyarwi1 tidak rasional, berarti DiA bukan org rasional ? Ini berarti yg memilih pak bos @prabowo atau @Gerindra Tidak RaSionaL, emangnya pake dengkul dan otot ?\n",
      "1597694394491994112 2022-11-29 20:50:02 +0000 <WartaEkonomi> Ganjar Tetap Moncer Nggak Peduli Siapa Cawapresnya, Charta Politika: Kalau dengan Prabowo Bisa Menang Satu Putaran  https://t.co/ImAVHKecYe\n",
      "1597693567316918272 2022-11-29 20:46:44 +0000 <SuratmanWirose1> @laki2enjoy @ListyoRini02 @AdeqOmpong4 @ekowboy2 Baru saja ada deklarasi mendukung Jokowi 3 periode oleh pendukung Jokowi dan Prabowo.  Kalau Jokowi bilang gak mau maju,  yg sering berlaku,   itu berbalik 180 derajat.\n",
      "1597693474291097600 2022-11-29 20:46:22 +0000 <Macanjkw> Prabowo memberi JANJI, Jokowi memberi BUKTI. Buka mata lebar-lebar, berfikir dengan akal sehat. Jangan terpengaruh isu SARA, kita memilih Presiden bukan Pemimpin Agama\n",
      "1597693364970827778 2022-11-29 20:45:56 +0000 <twitkuopiniku> @PartaiSocmed @aniesbaswedan Ngga belajar dari 2019 lalu sepertinya. Prabowo tetep aja keok meski sambutan dimana2.\n",
      "1597693361494044674 2022-11-29 20:45:55 +0000 <ArArozy> @Muhammad_Saewad Masih plin plan Alhamdulillah prabowo senyum lega. Ha-ha-ha 😀🙈🤣 awas prank pak\n",
      "1597693296263823360 2022-11-29 20:45:40 +0000 <pontederiacor> aaa mewek bgt grgr kisah hidup pak prabowo, gila setulus itu🥺\n",
      "1597692644930772992 2022-11-29 20:43:04 +0000 <WulandaPrabowo> Yaelaaah,emang dasar kelakuan.cowok tuh\n",
      "1597692172739891200 2022-11-29 20:41:12 +0000 <_SEKNAS_RI> @AgungSu76387898 @WAHYU_UNIFORM @CrbSira @Rasto66010876 @AwYesaya @Lembayung071 @dann84620377 @bambang20020901 @Thomastjik @BigDaddy234s @SangPejuank @erfinsyafrizal3 @Prihandono72 @rmiryanti @AriHend95106868 @SopanEdan @Metdro2 @anakranto64 @Pemberang1 @OrangBa13921152 @2020Kadrun @Cantika82104633 @borutoforhokage @harrycoocaa @Eddianto1 @Antaredja1 @SutayaLili @Budi_Dharma88 @BejoMulyo18 @mazkeboijo @ronaldo_juang @Nr53N74H7N @Pelangi32224458 @_NanaRosa__ @EffendiRistriy1 @kangwaqfi2 @EAbadi7 @hudattamini @ALIBABABETRESMI @Adzani03_Putri @AdrianROYALQ @abu_miftah13 @BocahKulon7 @barisoegriwa @ArekArusBawah @RonyAritonang3 @S4N_W1B1 @aniesbaswedan @prabowo @ganjarpranowo Senyumin aja, bung Anies #BapakPolitikIdentitas\n",
      "1597692038148808704 2022-11-29 20:40:40 +0000 <syahirularif> Lah maksudnya brgkl agar GP berpasangan dg Prabowo gitu.. paket komplit rambut putih ada kerutan di dahi😊\n",
      "1597690892009119744 2022-11-29 20:36:06 +0000 <Z_Abe_U> @firzahusainInc dua kali yg katanya ulama besar waktu itu mendukung pak Prabowo, 2 kali kalah!!! selama hatimu busuk, mau dimanapun kek kamu panjatkan doa tetap aja doamu busuk!!!!!\n",
      "1597689305048428544 2022-11-29 20:29:48 +0000 <MohQudsi9> Ayo Berwisata #NganjukKotaAngin  TPPJatim #TPPSumenep  @jokowi  @halimiskandarnu @yennywahid @prabowo @bclsinclair @NajwaShihab @Lesti_Jenong_DA @mohmahfudmd @gusmusgusmu @ganjarpranowo @agnezmo @AgusYudhoyono @muhaiminiskand2 @Suryafajar74  https://t.co/b5ZBueadbR\n",
      "1597687126732472320 2022-11-29 20:21:09 +0000 <JHPrabowo> @bro_aLy9 🤭🤭\n",
      "1597686363973120002 2022-11-29 20:18:07 +0000 <and_amewiya> Suka mikir sampe sekarang, dibalik Pak Prabowo gajadi jadi Presiden. Saking jujur dan mengintimidasi orang2 yang itu kali ya?\n",
      "1597685222027382784 2022-11-29 20:13:35 +0000 <prabowo_the> @FWBESS Gpp udah ayo gaskeun\n",
      "1597685084445765632 2022-11-29 20:13:02 +0000 <WulandaPrabowo> Janji setianya cowok tuh sama kayak janji dietnya cewek. Susah dipercaya\n",
      "1597684820800204800 2022-11-29 20:11:59 +0000 <prabowo_the> @FWBESS Wae i like this\n",
      "1597684563899166720 2022-11-29 20:10:58 +0000 <prabowo_the> @alterspaces Okey okey siap\n",
      "1597684436077715457 2022-11-29 20:10:27 +0000 <prabowo_the> @alterspaces Okey nanti di rate\n",
      "1597684361649741824 2022-11-29 20:10:10 +0000 <prabowo_the> @FWBESS Lah anyok\n",
      "1597684345975603201 2022-11-29 20:10:06 +0000 <SINDOnews> Jika Head to Head dengan Simulasi 2 Nama Capres, Prabowo Menang Lawan Siapa pun   #Sindonews #BukanBeritaBiasa . https://t.co/DUwsKwggcq\n",
      "1597684282729713664 2022-11-29 20:09:51 +0000 <prabowo_the> @Nrlfthm14 Makasih\n",
      "1597684201255448576 2022-11-29 20:09:31 +0000 <prabowo_the> @NSFWBESS_ Lanjut apa nih\n",
      "1597684034238173184 2022-11-29 20:08:51 +0000 <prabowo_the> @FWBESS Okey fine\n",
      "1597683907859689472 2022-11-29 20:08:21 +0000 <prabowo_the> @FWBESS Wah udah kelewat gak bisa dong\n",
      "1597683720168779776 2022-11-29 20:07:37 +0000 <prabowo_the> @FWBESS Tidur udah malam\n",
      "1597683485086748672 2022-11-29 20:06:41 +0000 <BagyaPrabowo> #Anime #中原麻衣 #中村悠一 #新井里美  https://t.co/TVSD8WOOd8\n",
      "1597683158081601537 2022-11-29 20:05:23 +0000 <prabowo_the> @FWBESS Yuk lah\n",
      "1597682822923268096 2022-11-29 20:04:03 +0000 <prabowo_the> @FWBESS Ayo\n",
      "1597682674579095552 2022-11-29 20:03:27 +0000 <prabowo_the> @FWBase18 Masih melek\n",
      "1597682641947410432 2022-11-29 20:03:20 +0000 <prabowo_the> @FWBESS Masih\n",
      "1597681787530260481 2022-11-29 19:59:56 +0000 <kikitteun> @kkod783027 @LGVany @malespake @smbdyto_u @Gerindra @tanyarlfes @prabowo \"nder, ify\"\n",
      "1597681613164290048 2022-11-29 19:59:14 +0000 <Otakuntilanak48> Walau ku sangat ingin bertemu, walau ku menyukaimu, kau jalan berlalu di depan mataku #SFK48\n",
      "1597680610428211200 2022-11-29 19:55:15 +0000 <Rismile3> Aku tadi liat vt² tentang pak Prabowo trus ngena bngt kata²nya \" jika aku harus mencintai dan berbagi hati itu hanya padamu, jika aku harus tanpamu akan tetap ku arungi hidup tanpa mencintai\". Gila dalem bangt ya cintanya beliau 😭\n",
      "1597680256328269825 2022-11-29 19:53:51 +0000 <BocahArusBawah> Disamping itu @Gerindra harus merombak para pentingnya yang tidak disukai oleh para pendukung @ganjarpranowo karna ditakutkan akan menggerogoti dari dalam.  Pasti siapa yang akan di rombak dari kubu @Gerindra pasti @prabowo paham dan mengerti.  Betulkan @bumnbersatu ?\n",
      "1597679638872223744 2022-11-29 19:51:24 +0000 <mynameisyours88> @opposite6892 Kadrun2. Udah 3 tahun berselang. Masih aja sakit hati kalah pilpres. Makin sakit hati lagi pak prabowo dan pak sandi join pemerintah. Tinggal PKS(Partai Kadrun Soak) yg jadi oposisi sama demokrat (DEMO BER KRAT2). Sono join Afghanistan sama Yaman aja biar di makmur\n",
      "1597679485842644992 2022-11-29 19:50:47 +0000 <BocahArusBawah> Jika @prabowo menjadi presiden&amp;wapres nya @ganjarpranowo pastinya pendukung @ganjarpranowo tidak  Terima, bisa jadi malah pendukung @ganjarpranowo berbalik arah&amp; @prabowo harus mengalah karna faktor adl kalah jumlah pendukung. Mengalah nya bukan karna kalah tapi demi NKRI MERDEKA\n",
      "1597679281383022592 2022-11-29 19:49:58 +0000 <AgungKuswanto19> @1febri @M45Broo Gak ada yang ngusung.. deketin prabowo.. wkwkwk.. Bro broo..\n",
      "1597678306488643584 2022-11-29 19:46:06 +0000 <SINDOnews> Survei: Prabowo Bisa Menang di Pilpres 2024 Jika Head to Head dengan Ganjar atau Anies   #Sindonews #BukanBeritaBiasa . https://t.co/KNIrqLtG4m\n",
      "1597678124623593473 2022-11-29 19:45:22 +0000 <RajaFadhil> @prabowo @Dahnilanzar Rambut  Pak @prabowo udah putih belum Om @Dahnilanzar?\n",
      "1597677986156728321 2022-11-29 19:44:49 +0000 <BocahArusBawah> meskipun begitu @prabowo harus mengalah menjadi cawapres dari @ganjarpranowo karna bisa membekingi pergerakan @ganjarpranowo untuk membumihanguskan para mafia dan bisa jadi @prabowo bisa naik bintang 5 setara soeharto , jenderal Sudirman dan jenderal ah nasution\n",
      "1597676920128561153 2022-11-29 19:40:35 +0000 <KingGaseng> Iki nek ra draw buakalane perang dunia 3\n",
      "1597676463473315840 2022-11-29 19:38:46 +0000 <BocahArusBawah> Dengan gambaran ini saja jika @ganjarpranowo bergandengan maju bersama @prabowo , bisa dipastikan PILPRES 2024 hanya 1 putaran meraup 58% dan dimenangkan @ganjarpranowo @prabowo  https://t.co/lKgFG5zpIr\n",
      "1597674849069330433 2022-11-29 19:32:22 +0000 <jQjIWj61LkKxOYF> Kayaknya @prabowo juga masuk dalam sebutan...?\n",
      "1597674690373615616 2022-11-29 19:31:44 +0000 <PuguhAjiPrabowo> lagi dan lagi aku dicintai seorang wanita sedih gue.. gue tu males cinta cintaan... nikah juga males kalau ngewek hayuuukk 🤣\n",
      "1597674310747172864 2022-11-29 19:30:13 +0000 <icallaci88> @Miduk17 Prabowo DI BAWAH ganjar nih ceritanya bong ??   @prabowo @Gerindra\n",
      "1597674262831443968 2022-11-29 19:30:02 +0000 <WartaEkonomi> Jokowi Sudah Pastikan: Saya Cek Pak Prabowo, Kerutan Wajahnya Ada, Rambutnya Juga Ada Putihnya  https://t.co/FCP1ivJgdw\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply() #digunakan sekali untuk mengaktifkan tindakan serentak dalam notebook jupyter.\n",
    "import twint #untuk import twint\n",
    "c = twint.Config()\n",
    "c.Search = 'prabowo'\n",
    "c.Pandas = True\n",
    "c.Limit = 70\n",
    "twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1669758886313,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "AW4keKjdDkwm",
    "outputId": "9b8436ef-8c96-4352-9296-38da97b2655b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     @WagimanDeep212_ ya udah @prabowo + @ganjarpra...\n",
       "1                          @prabowo Salam sejahtera BPK\n",
       "2     Anda coba telusuri deh.janji anies ke prabowo,...\n",
       "3     Alun-alun Nganjuk  #NganjukKotaAngin  TPPJatim...\n",
       "4     Alun-alun Nganjuk  #NganjukKotaAngin  TPPJatim...\n",
       "                            ...                        \n",
       "75    Dengan gambaran ini saja jika @ganjarpranowo b...\n",
       "76       Kayaknya @prabowo juga masuk dalam sebutan...?\n",
       "77    lagi dan lagi aku dicintai seorang wanita sedi...\n",
       "78    @Miduk17 Prabowo DI BAWAH ganjar nih ceritanya...\n",
       "79    Jokowi Sudah Pastikan: Saya Cek Pak Prabowo, K...\n",
       "Name: tweet, Length: 80, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tweets_dfs = twint.storage.panda.Tweets_df\n",
    "Tweets_dfs[\"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1669758886314,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "H_YZ8M09DnZG"
   },
   "outputs": [],
   "source": [
    "Tweets_dfs[\"tweet\"].to_csv(\"prabowo.csv\")\n",
    "from google.colab import files \n",
    "#files.download('prabowo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfhORp51vW00"
   },
   "source": [
    "# **Mengambil Data**\n",
    "\n",
    "Proses ini digunakan untuk mengambil 100 data tweet yang telah disimpan dalam github dengan format .csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3040,
     "status": "ok",
     "timestamp": 1669758889344,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "oSIOdhmaLkMf",
    "outputId": "2222b8be-8e00-4ae8-f717-55976ae182dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "#install library pandas\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3680,
     "status": "ok",
     "timestamp": 1669758893001,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "jdbqJE7nLn0f",
    "outputId": "db6b010d-8ceb-4843-f412-6e4dbc5f31d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n"
     ]
    }
   ],
   "source": [
    "#install library numpy\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "executionInfo": {
     "elapsed": 38,
     "status": "error",
     "timestamp": 1669758893003,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "Y_QNdvwcLt8B",
    "outputId": "0acef157-18cb-41b8-bc6b-2bee183e1314"
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f9220dddbedb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata_abstrak\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://raw.githubusercontent.com/SalmanAl27/webmining/master/prabowo.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdata_abstrak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# assuming storage_options is to be interpreted as headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mreq_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "data_abstrak = pd.read_csv(\"https://raw.githubusercontent.com/SalmanAl27/webmining/master/prabowo.csv\")\n",
    "\n",
    "data_abstrak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 35,
     "status": "aborted",
     "timestamp": 1669758893005,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "rlV9RgKLkOHg"
   },
   "outputs": [],
   "source": [
    "#install library sastrawi\n",
    "!pip install sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "aborted",
     "timestamp": 1669758893007,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "2owXZ4DqnbEu"
   },
   "outputs": [],
   "source": [
    "#install library swifter\n",
    "!pip install swifter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdrHBoVoxWj-"
   },
   "source": [
    "# **Case Folding**\n",
    "\n",
    "Tahap untuk merubah teks yang memiliki huruf kapital menjadi huruf kecil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "aborted",
     "timestamp": 1669758893009,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "bZb_8O0OMBc5"
   },
   "outputs": [],
   "source": [
    "data_abstrak['abstrak'] = data_abstrak['abstrak'].str.lower()\n",
    "\n",
    "\n",
    "data_abstrak['abstrak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 41,
     "status": "aborted",
     "timestamp": 1669758893012,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "GrS6ADokZLfV"
   },
   "outputs": [],
   "source": [
    "#install library nltk\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GidSZu_2ymdP"
   },
   "source": [
    "## Menghapus Karakter Spesial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "aborted",
     "timestamp": 1669758893013,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "7PfMxOqmamxD"
   },
   "outputs": [],
   "source": [
    "import string \n",
    "import re #regex library\n",
    "# import word_tokenize & FreqDist from NLTK\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# ------ Tokenizing ---------\n",
    "\n",
    "def remove_special(text):\n",
    "    # remove tab, new line, ans back slice\n",
    "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\" \").replace('\\\\f',\" \").replace('\\\\r',\" \")\n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove mention, link, hashtag\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "    # remove incomplete URL\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "                \n",
    "data_abstrak['abstrak'] = data_abstrak['abstrak'].apply(remove_special)\n",
    "data_abstrak['abstrak']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aaueWDLzwNm"
   },
   "source": [
    "## Menghapus Angka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "aborted",
     "timestamp": 1669758893014,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "nWoNw6CsywxF"
   },
   "outputs": [],
   "source": [
    "\n",
    "#remove number\n",
    "def remove_number(text):\n",
    "    return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "data_abstrak['abstrak'] = data_abstrak['abstrak'].apply(remove_number)\n",
    "data_abstrak['abstrak']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pY-4fnONz2Es"
   },
   "source": [
    "## Menghapus Tanda Baca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "aborted",
     "timestamp": 1669758893015,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "iCr9v6zey0QJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "data_abstrak['abstrak'] = data_abstrak['abstrak'].apply(remove_punctuation)\n",
    "data_abstrak['abstrak']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQQ9-N1_0Bvz"
   },
   "source": [
    "## Menghapus Spasi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 44,
     "status": "aborted",
     "timestamp": 1669758893017,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "BFo_LuODy2n0"
   },
   "outputs": [],
   "source": [
    "\n",
    "#remove whitespace leading & trailing\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "data_abstrak['abstrak'] = data_abstrak['abstrak'].apply(remove_whitespace_LT)\n",
    "\n",
    "\n",
    "#remove multiple whitespace into single whitespace\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "data_abstrak['abstrak'] = data_abstrak['abstrak'].apply(remove_whitespace_multiple)\n",
    "data_abstrak['abstrak']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNKKQN460Vz0"
   },
   "source": [
    "## Menghapus huruf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 46,
     "status": "aborted",
     "timestamp": 1669758893019,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "0UWUcE1ly9M1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# remove single char\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \" \", text)\n",
    "\n",
    "data_abstrak['abstrak'] = data_abstrak['abstrak'].apply(remove_singl_char)\n",
    "data_abstrak['abstrak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "aborted",
     "timestamp": 1669758893020,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "T30oghHxx2En"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncy4Kc0h0gjs"
   },
   "source": [
    "# **Tokenizing**\n",
    "\n",
    "Tokenizing adalah proses pemisahan teks menjadi potongan-potongan yang disebut sebagai token untuk kemudian di analisa. Kata, angka, simbol, tanda baca dan entitas penting lainnya dapat dianggap sebagai token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 48,
     "status": "aborted",
     "timestamp": 1669758893021,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "RrAM55OOy-t9"
   },
   "outputs": [],
   "source": [
    "\n",
    "# NLTK word Tokenize \n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "data_abstrak['abstrak'] = data_abstrak['abstrak'].apply(word_tokenize_wrapper)\n",
    "data_abstrak['abstrak']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRII2cK-1XPc"
   },
   "source": [
    "# **Filtering(Stopwords Removal)**\n",
    "Proses untuk menghapus kata hubung atau kata yang tidak memiliki makna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "aborted",
     "timestamp": 1669758893023,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "HyR-Rhpnijqv"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 50,
     "status": "aborted",
     "timestamp": 1669758893024,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "A9QuAKmgZ6py"
   },
   "outputs": [],
   "source": [
    "\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "#Menghapus Stopword dari list token\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "data_abstrak['abstrak'] = data_abstrak['abstrak'].apply(stopwords_removal)\n",
    "\n",
    "data_abstrak['abstrak']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuqMFVZa3H_d"
   },
   "source": [
    "# **Stemming**\n",
    "Data training hasil dari filtering akan dilakukan pengecekan atau pencarian kata-kata yang sesuai dengan kamus umum. Apabila data training hasil filtering sesuai dengan kamus umum maka kata akan dikeluarkan sementara, karena sudah dianggap sebagai kata dasar. Apabila masih terdapat kata yang tidak termasuk dalam kata dasar maka tahap selanjutnya adalah menghapus inflection suffixes yang merupakan akhiran pertama. Kata yang memiliki akhiran partticles seperti “-pun”, “-kah”, “-tah”, “- lah” dan akhiran possessive pronoun seperti “-mu”, “-ku” dan “-nya” dihilangkan. Setelah dilakukan proses case folding, tokenezing, dan filtering, proses selanjutnya yaitu stemming. Stemming yang digunakan pada penelitian ini menggunakan algoritma Enhanced Confix Stipping Stemmer, terdiri dari beberapa langkah: Data training hasil dari filtering akan dilakukan pengecekan atau pencarian kata-kata yang sesuai dengan kamus umum. Apabila data training hasil filtering sesuai dengan kamus umum maka kata akan dikeluarkan sementara, karena sudah dianggap sebagai kata dasar. Apabila masih terdapat kata yang tidak termasuk dalam kata dasar maka tahap selanjutnya adalah menghapus inflection suffixes yang merupakan akhiran pertama. Kata yang memiliki akhiran partticles seperti “-pun”, “-kah”, “-tah”, “- lah” dan akhiran possessive pronoun seperti “-mu”, “-ku” dan “-nya” dihilangkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 55,
     "status": "aborted",
     "timestamp": 1669758893029,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "NIUkj010adSU"
   },
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import swifter\n",
    "\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "\n",
    "for document in data_abstrak['abstrak']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "            \n",
    "print(len(term_dict))\n",
    "print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "aborted",
     "timestamp": 1669758893031,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "2Y4s8PsVcZK-"
   },
   "outputs": [],
   "source": [
    "\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "    print(term,\":\" ,term_dict[term])\n",
    "    \n",
    "print(term_dict)\n",
    "print(\"------------------------\")\n",
    "\n",
    "\n",
    "# apply stemmed term to dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "data_abstrak['abstrak'] = data_abstrak['abstrak'].swifter.apply(get_stemmed_term)\n",
    "data_abstrak['abstrak']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfa1PWdV4RC9"
   },
   "source": [
    "##Menyimpan Hasil Tahap Preprocessing ke file .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 61,
     "status": "aborted",
     "timestamp": 1669758893036,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "1IO8wy-TV8Dk"
   },
   "outputs": [],
   "source": [
    "\n",
    "#data_abstrak.to_csv('preprocessing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KF8PKplK4pqu"
   },
   "source": [
    "# **TF**\n",
    "\n",
    "TF(Term Frequency) : Istilah frekuensi kata dalam dokumen. Ada beberapa cara untuk menghitung frekuensi ini, dengan cara yang paling sederhana adalah dengan menghitung jumlah kata yang muncul dalam dokumen. Lalu, ada cara untuk menyesuaikan frekuensi, berdasarkan panjang dokumen, atau dengan frekuensi mentah kata yang paling sering muncul dalam dokumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "aborted",
     "timestamp": 1669758893038,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "0Qda3OpcSCq2"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer\n",
    "#Membuat Dataframe\n",
    "dataTextPre = pd.read_csv('preprocessing.csv')\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "bag = vectorizer.fit_transform(dataTextPre['abstrak'])\n",
    "dataTextPre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 65,
     "status": "aborted",
     "timestamp": 1669758893040,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "L6hjdegpSYqN"
   },
   "outputs": [],
   "source": [
    "matrik_vsm=bag.toarray()\n",
    "#print(matrik_vsm)\n",
    "matrik_vsm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 66,
     "status": "aborted",
     "timestamp": 1669758893041,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "D4iujuMwS0It"
   },
   "outputs": [],
   "source": [
    "matrik_vsm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 68,
     "status": "aborted",
     "timestamp": 1669758893043,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "M6GcpBXy8cYI"
   },
   "outputs": [],
   "source": [
    "a=vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 70,
     "status": "aborted",
     "timestamp": 1669758893045,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "_RL5lPUR8d6G"
   },
   "outputs": [],
   "source": [
    "print(len(matrik_vsm[:,1]))\n",
    "#dfb =pd.DataFrame(data=matrik_vsm,index=df,columns=[a])\n",
    "dataTF =pd.DataFrame(data=matrik_vsm,index=list(range(1, len(matrik_vsm[:,1])+1, )),columns=[a])\n",
    "dataTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 71,
     "status": "aborted",
     "timestamp": 1669758893046,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "Y7mv3OjJoV2Q"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer\n",
    "DataTFIDF = TfidfVectorizer()\n",
    "TFIDF = DataTFIDF.fit_transform(dataTextPre['abstrak']).toarray()\n",
    "TFIDF = pd.DataFrame(TFIDF)\n",
    "TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oY6Vxq9qphjh"
   },
   "source": [
    "# KMeans(Clustering)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYmNkyUzplf5"
   },
   "source": [
    "K-Means Clustering adalah suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan unssupervised learning dan menggunakan metode yang mengelompokan data berbagai partisi.\n",
    "\n",
    "K Means Clustering memiliki objective yaitu meminimalisasi object function yang telah di atur pada proses clasterisasi. Dengan cara minimalisasi variasi antar 1 cluster dengan maksimalisasi variasi dengan data di cluster lainnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 72,
     "status": "aborted",
     "timestamp": 1669758893047,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "GLWjSWS7wBmj"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans =KMeans(n_clusters=3)\n",
    "kmeans=kmeans.fit(dataTF)\n",
    "prediksi=kmeans.predict(dataTF)\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "data=pd.DataFrame(prediksi,columns=[\"Cluster\"])\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kCJnztlp_6J"
   },
   "source": [
    "Menambah Label pada Hasil TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 73,
     "status": "aborted",
     "timestamp": 1669758893048,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "r4IQgGxNoUJG"
   },
   "outputs": [],
   "source": [
    "datalabel = pd.read_csv('https://raw.githubusercontent.com/SalmanAl27/webmining/master/prabowo.csv')\n",
    "dataJurnal = pd.concat([dataTF.reset_index(drop=True), datalabel[\"label\"]], axis=1)\n",
    "dataJurnal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 76,
     "status": "aborted",
     "timestamp": 1669758893052,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "4hlIbmOvsOfd"
   },
   "outputs": [],
   "source": [
    "dataJurnal['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 77,
     "status": "aborted",
     "timestamp": 1669758893053,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "4oOxuqzzspqD"
   },
   "outputs": [],
   "source": [
    "dataJurnal.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dsiaV8fqNAK"
   },
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 79,
     "status": "aborted",
     "timestamp": 1669758893055,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "_R05JYzZsn39"
   },
   "outputs": [],
   "source": [
    "### Train test split to avoid overfitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(dataJurnal.drop(labels=['label'], axis=1),\n",
    "    dataJurnal['label'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 80,
     "status": "aborted",
     "timestamp": 1669758893056,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "jc35b3YmtBFW"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xd_u54nVqcuJ"
   },
   "source": [
    "# KNN (Analisis Sentimen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgMwLhPDqWe6"
   },
   "source": [
    "KNN adalah model sederhana untuk tugas-tugas regresi dan klasifikasi. Ketetanggan adalah representasi dari contoh pelatihan dalam ruang metrik. Ruang metrik adalah ruang fitur di mana jarak antara semua anggota set didefinisikan. Berkaitan dengan masalah pizza pada bab sebelumnya, contoh data latih diwakili dalam ruang metrik karena jarak antara semua diameter pizza ditentukan. Ketetanggan ini digunakan untuk memperkirakan nilai variabel respon untuk contoh uji. Hyperparameter k menentukan berapa banyak tetangga yang dapat digunakan dalam estimasi. Hyperparameter adalah parameter yang mengontrol bagaimana algoritma belajar; hiperparameter tidak diperkirakan dari data pelatihan dan kadang-kadang ditetapkan secara manual. Akhirnya, k tetangga yang dipilih adalah yang terdekat dengan instan uji, yang diukur dengan beberapa fungsi jarak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 81,
     "status": "aborted",
     "timestamp": 1669758893057,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "TTIp-NYKOKm8"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "testing=[]\n",
    "listnum=[]\n",
    "for i in range(2,21):\n",
    "  listnum.append(i)\n",
    "  neigh = KNeighborsClassifier(n_neighbors=i)\n",
    "  neigh.fit(X_train, y_train)\n",
    "  Y_pred = neigh.predict(X_test) \n",
    "  testing.append(Y_pred)\n",
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 81,
     "status": "aborted",
     "timestamp": 1669758893058,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "GinsZ2_4P-Yq"
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUc3qesDOMmC"
   },
   "source": [
    "##hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 82,
     "status": "aborted",
     "timestamp": 1669758893059,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "6Fjfhu5eRhs5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, accuracy_score,precision_score\n",
    "listtest=[]\n",
    "listacc=[]\n",
    "for i in range(len(testing)):\n",
    "  accuracy_neigh=round(accuracy_score(y_test,testing[i])* 100, 2)\n",
    "  acc_neigh = round(neigh.score(X_train, y_train) * 100, 2)\n",
    "  listappend=listnum[i]\n",
    "  appendlist=listappend,accuracy_neigh\n",
    "  listtest.append(appendlist)\n",
    "  listacc.append(accuracy_neigh)\n",
    "listtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 84,
     "status": "aborted",
     "timestamp": 1669758893061,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "mL3BVmvUuVQ7"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.bar(listnum, listacc)\n",
    "plt.xticks(listnum)\n",
    "plt.title('Nilai Akurasi Berdasarkan Input')\n",
    "plt.ylabel('Persentase Akurasi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Tgkfr3gr1yZ"
   },
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAtPFS4-LIBq"
   },
   "source": [
    "Tipic Modeling adalah cara mengelompokan data text berdasarkan suatu topik tertentu. Memiliki tujuan yang sama dengan klasifikasi tetapi menggunakan pendekatan berbeda topik modelling merupakan unsupervised learning alias tidak membutuhkan data berlabel. bisa dikatakan topic modelling bekerja seperti clustering dengan mengelompokan dokumen berdasarkan kemiripanya, tetapi topic modelling mempunyai tujuan yang lebih spesifik yaitu :\n",
    "\n",
    "1. Menemukan pola topik abstrak pada kumpulan dokumen\n",
    "2. Memberikan anotasi dokumen berdasarkan topik tersebut\n",
    "3. Menggunakan Anotasi dokumen untuk mengelompokan dokumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0GeuLB2tvNr"
   },
   "source": [
    "## Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uparZeoGsDaw"
   },
   "source": [
    "Mutual Information atau Information Gain adalah salah satu metode dari seleksi fitur, dalam proses Information Gain fitur akan diranking, ranking fitur yang terbesar merupakan fitur yang paling relevan dan memiliki koneksi yang kuat dengan kumpulan data yang terkait "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 85,
     "status": "aborted",
     "timestamp": 1669758893062,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "WWwlnB9rtFYs"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "# determine the mutual information\n",
    "mutual_info = mutual_info_classif(X_train, y_train)\n",
    "mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 87,
     "status": "aborted",
     "timestamp": 1669758893064,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "7Pw-KWsstTeM"
   },
   "outputs": [],
   "source": [
    "mutual_info = pd.Series(mutual_info)\n",
    "mutual_info.index = X_train.columns\n",
    "mutual_info.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 88,
     "status": "aborted",
     "timestamp": 1669758893065,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "FUpXiR8IxsFX"
   },
   "outputs": [],
   "source": [
    "#let's plot the ordered mutual_info values per feature\n",
    "mutual_info.sort_values(ascending=False).plot.bar(figsize=(20, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 48238,
     "status": "aborted",
     "timestamp": 1669758893067,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "ZKm54BR5x2FV"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 48234,
     "status": "aborted",
     "timestamp": 1669758893069,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "3uCOqr99x7vV"
   },
   "outputs": [],
   "source": [
    "#No we Will select the  top 5 important features\n",
    "sel_five_cols = SelectKBest(mutual_info_classif, k=100)\n",
    "sel_five_cols.fit(X_train, y_train)\n",
    "X_train.columns[sel_five_cols.get_support()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCjReKPjna-p"
   },
   "source": [
    "Model LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 48230,
     "status": "aborted",
     "timestamp": 1669758893070,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "I-U8g1SH7BmH"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "#Membuat LSA Model\n",
    "lsa_model = TruncatedSVD(n_components=10, algorithm='randomized', n_iter=10, random_state=42)\n",
    "#Ft Transform LSA Model dari TF-IDF\n",
    "lsa_top=lsa_model.fit_transform(TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 48226,
     "status": "aborted",
     "timestamp": 1669758893071,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "Lx0mu_QangA7"
   },
   "outputs": [],
   "source": [
    "#Menampilkan LSA top 10\n",
    "print(lsa_top)\n",
    "print(lsa_top.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSVTtFn0pHOV"
   },
   "source": [
    "Menampilkan Kekuatan Topik pada Setiap Dokumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 48222,
     "status": "aborted",
     "timestamp": 1669758893072,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "V99gP1ygokp0"
   },
   "outputs": [],
   "source": [
    "for Dokumen in range(TFIDF.shape[0]):\n",
    "    print(\"\\nDokumen : \", Dokumen)\n",
    "    l=lsa_top[Dokumen]\n",
    "    for i,topik in enumerate(l):\n",
    "        print(\"Topik \",i,\" : \",topik*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4j2Labm3pPRS"
   },
   "source": [
    "Mengidentifikasi Komponen Kata Setiap Topik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 48218,
     "status": "aborted",
     "timestamp": 1669758893073,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "pACaNF_Bor7I"
   },
   "outputs": [],
   "source": [
    "print(lsa_model.components_.shape) # (no_of_topics*no_of_words)\n",
    "print(lsa_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 48214,
     "status": "aborted",
     "timestamp": 1669758893074,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "3Jyk4dNIox0U"
   },
   "outputs": [],
   "source": [
    "# Kata penting dari setiap Topik\n",
    "vocab = DataTFIDF.get_feature_names()\n",
    "\n",
    "for i, comp in enumerate(lsa_model.components_):\n",
    "    vocab_comp = zip(vocab, comp)\n",
    "    #Menampilkan 10 kata penting setiap topik\n",
    "    sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "    print(\"Topik \"+str(i)+\": \")\n",
    "    for t in sorted_words:\n",
    "        print(t[0],end=\" \")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUx2Rp60KpNb"
   },
   "source": [
    "# Ensemble Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQPInjL93Yuv"
   },
   "source": [
    "Metode ensemble atau metode ansamble adalah algoritma dalam pembelajaran mesin (machine learning) dimana algoritma ini sebagai pencarian solusi prediksi terbaik dibandingkan dengan algoritma yang lain karena metode ensemble ini menggunakan beberapa algoritma pembelajaran untuk pencapaian solusi prediksi yang lebih baik daripada algoritma yang bisa diperoleh dari salah satu pembelajaran algoritma kosituen saja. Tidak seperti ansamble statistika didalam mekanika statistika biasanya selalu tak terbatas. Ansemble Pembelajaran hanya terdiri dari seperangkat model alternatif yang bersifat terbatas, namun biasanya memungkinkan untuk menjadi lebih banyak lagi struktur fleksibel yang ada diantara alternatif model itu sendiri.\n",
    "\n",
    "Evaluasi prediksi dari ensemble biasanya memerlukan banyak komputasi daripada evaluasi prediksi model tunggal (single model), jadi ensemble ini memungkinkan untuk mengimbangi poor learning algorithms oleh performasi lebih dari komputasi itu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 48210,
     "status": "aborted",
     "timestamp": 1669758893075,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "PtrkKHsa3b1F"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "estimators = [\n",
    "    ('rf1', RandomForestClassifier(n_estimators=10, random_state=40)),\n",
    "    ('rf2', RandomForestClassifier(n_estimators=5, random_state=40))\n",
    "    \n",
    "    ]\n",
    "clf = StackingClassifier(\n",
    "    estimators=estimators, final_estimator=LogisticRegression()\n",
    "    )\n",
    "\n",
    "clf.fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 48206,
     "status": "aborted",
     "timestamp": 1669758893076,
     "user": {
      "displayName": "19-132 Muhammad Salman Al Farisi",
      "userId": "16576458071114986096"
     },
     "user_tz": -420
    },
    "id": "uyxujVxB63WV"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GidSZu_2ymdP",
    "4aaueWDLzwNm",
    "pY-4fnONz2Es",
    "zQQ9-N1_0Bvz",
    "kNKKQN460Vz0",
    "6dsiaV8fqNAK",
    "VUc3qesDOMmC",
    "H0GeuLB2tvNr"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}